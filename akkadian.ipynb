{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14661932,"sourceType":"datasetVersion","datasetId":9366631},{"sourceId":14809604,"sourceType":"datasetVersion","datasetId":9470040},{"sourceId":297179109,"sourceType":"kernelVersion"},{"sourceId":691084,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":523956,"modelId":537969}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nos.environ[\"MKL_NUM_THREADS\"] = \"4\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\nos.environ[\"TORCH_CUDNN_V8_API_ENABLED\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nimport re\nimport random\nimport math\nfrom pathlib import Path\nfrom typing import List\nfrom collections import Counter\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nbanner_palette = [\n    \"#2c1810\",  # dark brown\n    \"#5c4a2a\",  # medium brown\n    \"#8b6914\",  # golden brown\n    \"#d4a843\",  # gold\n    \"#f0d68a\"   # light gold\n]\nsns.set_palette(banner_palette)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom torch.cuda.amp import autocast\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom tqdm.auto import tqdm\n\nprint('Done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:17.390453Z","iopub.execute_input":"2026-02-11T16:24:17.390678Z","iopub.status.idle":"2026-02-11T16:24:38.385410Z","shell.execute_reply.started":"2026-02-11T16:24:17.390656Z","shell.execute_reply":"2026-02-11T16:24:38.384589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Metrics for competition","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n\nvar=\"/kaggle/input/datasets/canicule/translate-akkadian-texts-wheel\"\n!pip install \\\n  \"$var\"/optuna-4.7.0-py3-none-any.whl \\\n  \"$var\"/sacrebleu-2.6.0-py3-none-any.whl \\\n  \"$var\"/portalocker-3.2.0-py3-none-any.whl \\\n  --no-index \\\n  --find-links \"$var\"\n\n#!pip install /kaggle/input/datasets/canicule/translate-akkadian-texts-wheel/optuna-4.7.0-py3-none-any.whl\n\n#clear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:38.387142Z","iopub.execute_input":"2026-02-11T16:24:38.387662Z","iopub.status.idle":"2026-02-11T16:24:44.988668Z","shell.execute_reply.started":"2026-02-11T16:24:38.387632Z","shell.execute_reply":"2026-02-11T16:24:44.987668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:44.990524Z","iopub.execute_input":"2026-02-11T16:24:44.990821Z","iopub.status.idle":"2026-02-11T16:24:44.994993Z","shell.execute_reply.started":"2026-02-11T16:24:44.990791Z","shell.execute_reply":"2026-02-11T16:24:44.994382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# METRICS: sacrebleu with pure-Python fallback\n# ============================================================\n'''\nUSE_SACREBLEU = False\ntry:\n    import sacrebleu\n    USE_SACREBLEU = True\n    print(\"sacrebleu loaded\")\nexcept ImportError:\n    try:\n        import subprocess, sys\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sacrebleu\", \"-q\"])\n        import sacrebleu\n        USE_SACREBLEU = True\n        print(\"sacrebleu installed and loaded\")\n    except Exception:\n        print(\"sacrebleu unavailable — using built-in BLEU/chrF++ implementation\")\n\ntry:\n    import optuna\n    print(\"optuna loaded\")\nexcept ImportError:\n    try:\n        import subprocess, sys\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\", \"-q\"])\n        import optuna\n        print(\"optuna installed and loaded\")\n    except Exception:\n        raise ImportError(\"optuna is required but could not be installed. Enable internet or pre-install optuna.\")\n\n'''\n\ndef _ngrams(tokens, n):\n    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n\ndef _corpus_bleu_fallback(hypotheses, references, max_n=4):\n    \"\"\"Simplified corpus BLEU (no smoothing, brevity penalty included).\"\"\"\n    clip_counts = [0] * max_n\n    total_counts = [0] * max_n\n    hyp_len = 0\n    ref_len = 0\n    for hyp, ref in zip(hypotheses, references):\n        hyp_tok = hyp.split()\n        ref_tok = ref.split()\n        hyp_len += len(hyp_tok)\n        ref_len += len(ref_tok)\n        for n in range(1, max_n + 1):\n            hyp_ng = Counter(_ngrams(hyp_tok, n))\n            ref_ng = Counter(_ngrams(ref_tok, n))\n            clipped = {ng: min(c, ref_ng.get(ng, 0)) for ng, c in hyp_ng.items()}\n            clip_counts[n-1] += sum(clipped.values())\n            total_counts[n-1] += max(len(hyp_tok) - n + 1, 0)\n    precisions = []\n    for n in range(max_n):\n        if total_counts[n] == 0:\n            precisions.append(0)\n        else:\n            precisions.append(clip_counts[n] / total_counts[n])\n    if any(p == 0 for p in precisions):\n        return 0.0\n    log_avg = sum(math.log(p) for p in precisions) / max_n\n    bp = 1.0 if hyp_len >= ref_len else math.exp(1 - ref_len / max(hyp_len, 1))\n    return bp * math.exp(log_avg) * 100\n\n\ndef _chrf_pp_fallback(hypotheses, references, n_char=6, n_word=2, beta=2):\n    \"\"\"Simplified chrF++ (character n-gram F-score + word n-grams).\"\"\"\n    total_hyp_ngrams = 0\n    total_ref_ngrams = 0\n    total_matches = 0\n    for hyp, ref in zip(hypotheses, references):\n        for n in range(1, n_char + 1):\n            hyp_ng = Counter(_ngrams(list(hyp), n))\n            ref_ng = Counter(_ngrams(list(ref), n))\n            matches = sum(min(hyp_ng[ng], ref_ng[ng]) for ng in hyp_ng if ng in ref_ng)\n            total_matches += matches\n            total_hyp_ngrams += sum(hyp_ng.values())\n            total_ref_ngrams += sum(ref_ng.values())\n        for n in range(1, n_word + 1):\n            hyp_ng = Counter(_ngrams(hyp.split(), n))\n            ref_ng = Counter(_ngrams(ref.split(), n))\n            matches = sum(min(hyp_ng[ng], ref_ng[ng]) for ng in hyp_ng if ng in ref_ng)\n            total_matches += matches\n            total_hyp_ngrams += sum(hyp_ng.values())\n            total_ref_ngrams += sum(ref_ng.values())\n    precision = total_matches / max(total_hyp_ngrams, 1)\n    recall = total_matches / max(total_ref_ngrams, 1)\n    if precision + recall == 0:\n        return 0.0\n    beta_sq = beta ** 2\n    f_score = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n    return f_score * 100\n\n\ndef _sentence_bleu_fallback(hypothesis, reference, max_n=4):\n    \"\"\"Sentence-level BLEU with add-1 smoothing.\"\"\"\n    hyp_tok = hypothesis.split()\n    ref_tok = reference.split()\n    precisions = []\n    for n in range(1, max_n + 1):\n        hyp_ng = Counter(_ngrams(hyp_tok, n))\n        ref_ng = Counter(_ngrams(ref_tok, n))\n        clipped = sum(min(c, ref_ng.get(ng, 0)) for ng, c in hyp_ng.items())\n        total = max(len(hyp_tok) - n + 1, 0)\n        precisions.append((clipped + 1) / (total + 1))  # add-1 smoothing\n    log_avg = sum(math.log(p) for p in precisions) / max_n\n    bp = 1.0 if len(hyp_tok) >= len(ref_tok) else math.exp(1 - len(ref_tok) / max(len(hyp_tok), 1))\n    return bp * math.exp(log_avg) * 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:44.996255Z","iopub.execute_input":"2026-02-11T16:24:44.996607Z","iopub.status.idle":"2026-02-11T16:24:45.017373Z","shell.execute_reply.started":"2026-02-11T16:24:44.996583Z","shell.execute_reply":"2026-02-11T16:24:45.016716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"print(f\"\\nPyTorch: {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} | {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.019340Z","iopub.execute_input":"2026-02-11T16:24:45.019588Z","iopub.status.idle":"2026-02-11T16:24:45.338106Z","shell.execute_reply.started":"2026-02-11T16:24:45.019566Z","shell.execute_reply":"2026-02-11T16:24:45.337315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading and analyzing data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/test.csv\")\nprint(\"- The train set's shape is\", df_train.shape[0], \"rows and\", df_train.shape[1], \"columns.\")\nprint(\"- The test set's shape is\", df_test.shape[0], \"rows and\", df_test.shape[1], \"columns.\")\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.339100Z","iopub.execute_input":"2026-02-11T16:24:45.339452Z","iopub.status.idle":"2026-02-11T16:24:45.467267Z","shell.execute_reply.started":"2026-02-11T16:24:45.339418Z","shell.execute_reply":"2026-02-11T16:24:45.466599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.468108Z","iopub.execute_input":"2026-02-11T16:24:45.468371Z","iopub.status.idle":"2026-02-11T16:24:45.493696Z","shell.execute_reply.started":"2026-02-11T16:24:45.468347Z","shell.execute_reply":"2026-02-11T16:24:45.492973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.494611Z","iopub.execute_input":"2026-02-11T16:24:45.494959Z","iopub.status.idle":"2026-02-11T16:24:45.521547Z","shell.execute_reply.started":"2026-02-11T16:24:45.494931Z","shell.execute_reply":"2026-02-11T16:24:45.520773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute text lengths\ndf_train['src_word_count'] = df_train['transliteration'].fillna('').apply(lambda x: len(x.split()))\ndf_train['tgt_word_count'] = df_train['translation'].fillna('').apply(lambda x: len(x.split()))\ndf_train['src_char_count'] = df_train['transliteration'].fillna('').str.len()\ndf_train['tgt_char_count'] = df_train['translation'].fillna('').str.len()\ndf_test['src_word_count'] = df_test['transliteration'].fillna('').apply(lambda x: len(x.split()))\ndf_test['src_char_count'] = df_test['transliteration'].fillna('').str.len()\n\nprint(\"Source (transliteration) word count stats:\")\nprint(df_train['src_word_count'].describe())\nprint(\"\\nTarget (translation) word count stats:\")\nprint(df_train['tgt_word_count'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.522829Z","iopub.execute_input":"2026-02-11T16:24:45.523209Z","iopub.status.idle":"2026-02-11T16:24:45.572674Z","shell.execute_reply.started":"2026-02-11T16:24:45.523168Z","shell.execute_reply":"2026-02-11T16:24:45.572085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Source word count\naxes[0, 0].hist(df_train['src_word_count'], bins=50, color='#8b6914', alpha=0.7, edgecolor='#2c1810')\naxes[0, 0].axvline(df_train['src_word_count'].mean(), color='#d4a843', linestyle='--', linewidth=2, label=f\"Mean: {df_train['src_word_count'].mean():.1f}\")\naxes[0, 0].axvline(df_train['src_word_count'].median(), color='#f0d68a', linestyle='-.', linewidth=2, label=f\"Median: {df_train['src_word_count'].median():.1f}\")\naxes[0, 0].set_title('Source (Transliteration) Word Count', fontweight='bold')\naxes[0, 0].legend()\n\n# Target word count\naxes[0, 1].hist(df_train['tgt_word_count'], bins=50, color='#5c4a2a', alpha=0.7, edgecolor='#2c1810')\naxes[0, 1].axvline(df_train['tgt_word_count'].mean(), color='#d4a843', linestyle='--', linewidth=2, label=f\"Mean: {df_train['tgt_word_count'].mean():.1f}\")\naxes[0, 1].axvline(df_train['tgt_word_count'].median(), color='#f0d68a', linestyle='-.', linewidth=2, label=f\"Median: {df_train['tgt_word_count'].median():.1f}\")\naxes[0, 1].set_title('Target (Translation) Word Count', fontweight='bold')\naxes[0, 1].legend()\n\n# Source character count\naxes[1, 0].hist(df_train['src_char_count'], bins=50, color='#8b6914', alpha=0.7, edgecolor='#2c1810')\naxes[1, 0].set_title('Source Character Count', fontweight='bold')\n\n# Target character count\naxes[1, 1].hist(df_train['tgt_char_count'], bins=50, color='#5c4a2a', alpha=0.7, edgecolor='#2c1810')\naxes[1, 1].set_title('Target Character Count', fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:45.573561Z","iopub.execute_input":"2026-02-11T16:24:45.573878Z","iopub.status.idle":"2026-02-11T16:24:46.537658Z","shell.execute_reply.started":"2026-02-11T16:24:45.573853Z","shell.execute_reply":"2026-02-11T16:24:46.537039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Word count comparison\nsns.kdeplot(df_train['src_word_count'], ax=axes[0], label='Train', fill=True, alpha=0.5, color='#8b6914')\nsns.kdeplot(df_test['src_word_count'], ax=axes[0], label='Test', fill=True, alpha=0.3, color='#d4a843')\naxes[0].set_title('Source Word Count: Train vs Test')\naxes[0].legend()\n\n# Character count comparison\nsns.kdeplot(df_train['src_char_count'], ax=axes[1], label='Train', fill=True, alpha=0.5, color='#8b6914')\nsns.kdeplot(df_test['src_char_count'], ax=axes[1], label='Test', fill=True, alpha=0.3, color='#d4a843')\naxes[1].set_title('Source Char Count: Train vs Test')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:46.538848Z","iopub.execute_input":"2026-02-11T16:24:46.539155Z","iopub.status.idle":"2026-02-11T16:24:46.994209Z","shell.execute_reply.started":"2026-02-11T16:24:46.539120Z","shell.execute_reply":"2026-02-11T16:24:46.993477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(df_train['src_word_count'], df_train['tgt_word_count'], alpha=0.3, color='#8b6914', s=10)\nplt.xlabel('Source Word Count (Transliteration)')\nplt.ylabel('Target Word Count (Translation)')\nplt.title('Source vs Target Length Relationship')\n\n# Fit and plot trend line\nz = np.polyfit(df_train['src_word_count'], df_train['tgt_word_count'], 1)\np = np.poly1d(z)\nx_line = np.linspace(0, df_train['src_word_count'].max(), 100)\nplt.plot(x_line, p(x_line), color='#d4a843', linewidth=2, linestyle='--', label=f'Trend: y={z[0]:.2f}x + {z[1]:.2f}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"Correlation between source and target word counts: {df_train['src_word_count'].corr(df_train['tgt_word_count']):.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:46.995214Z","iopub.execute_input":"2026-02-11T16:24:46.995562Z","iopub.status.idle":"2026-02-11T16:24:47.246495Z","shell.execute_reply.started":"2026-02-11T16:24:46.995527Z","shell.execute_reply":"2026-02-11T16:24:47.245757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze gap markers in source texts\ndf_train['has_gap'] = df_train['transliteration'].fillna('').str.contains(r'\\bx\\b|xx|\\.\\.\\.|…', regex=True)\ndf_test['has_gap'] = df_test['transliteration'].fillna('').str.contains(r'\\bx\\b|xx|\\.\\.\\.|…', regex=True)\n\nprint(f\"Train texts with gaps: {df_train['has_gap'].sum()} ({df_train['has_gap'].mean()*100:.1f}%)\")\nprint(f\"Test texts with gaps:  {df_test['has_gap'].sum()} ({df_test['has_gap'].mean()*100:.1f}%)\")\n\n# Count gap markers per text\ndf_train['gap_count'] = df_train['transliteration'].fillna('').apply(\n    lambda x: len(re.findall(r'\\bx\\b|xx+|\\.\\.\\.|…', x))\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Gap presence pie chart\ncounts = df_train['has_gap'].value_counts()\nlabels = ['No Gaps', 'Has Gaps']\ncolors = ['#8b6914', '#d4a843']\naxes[0].pie(counts, labels=labels, colors=colors, autopct='%1.1f%%',\n            textprops={'fontsize': 12, 'fontweight': 'bold'})\naxes[0].set_title('Gap Marker Presence in Train', fontweight='bold')\n\n# Gap count distribution\naxes[1].hist(df_train[df_train['gap_count'] > 0]['gap_count'], bins=30, \n             color='#8b6914', alpha=0.7, edgecolor='#2c1810')\naxes[1].set_title('Gap Count Distribution (texts with gaps)', fontweight='bold')\naxes[1].set_xlabel('Number of Gap Markers')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:47.247540Z","iopub.execute_input":"2026-02-11T16:24:47.248097Z","iopub.status.idle":"2026-02-11T16:24:47.552632Z","shell.execute_reply.started":"2026-02-11T16:24:47.248071Z","shell.execute_reply":"2026-02-11T16:24:47.552013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Most common words in target translations\nall_target_words = ' '.join(df_train['translation'].fillna('')).lower().split()\nword_counts = Counter(all_target_words)\ntop_30 = word_counts.most_common(30)\n\nplt.figure(figsize=(14, 6))\nwords, counts_list = zip(*top_30)\nplt.bar(words, counts_list, color='#8b6914', edgecolor='#2c1810')\nplt.xticks(rotation=45, ha='right')\nplt.title('Top 30 Most Common Words in English Translations', fontweight='bold')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:47.555357Z","iopub.execute_input":"2026-02-11T16:24:47.555606Z","iopub.status.idle":"2026-02-11T16:24:47.865139Z","shell.execute_reply.started":"2026-02-11T16:24:47.555583Z","shell.execute_reply":"2026-02-11T16:24:47.864310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['src_word_count', 'tgt_word_count']:\n    Q1 = df_train[col].quantile(0.25)\n    Q3 = df_train[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = df_train[(df_train[col] < lower_bound) | (df_train[col] > upper_bound)]\n    print(f\"{col}: Lower={lower_bound:.0f}, Upper={upper_bound:.0f}, Outliers={outliers.shape[0]}\")\n\n# Length ratio analysis\ndf_train['length_ratio'] = df_train['tgt_word_count'] / df_train['src_word_count'].clip(lower=1)\nprint(f\"\\nLength ratio (target/source) stats:\")\nprint(df_train['length_ratio'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:47.866174Z","iopub.execute_input":"2026-02-11T16:24:47.866523Z","iopub.status.idle":"2026-02-11T16:24:47.882815Z","shell.execute_reply.started":"2026-02-11T16:24:47.866492Z","shell.execute_reply":"2026-02-11T16:24:47.881944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# PREPROCESSOR (exact match: chunky_v1_5_0)\n# ============================================================\n\nclass OptimizedPreprocessor:\n    def __init__(self):\n        self.patterns = {\n            'big_gap': re.compile(r'(\\.{3,}|…+|……)'),\n            'small_gap': re.compile(r'(xx+|\\s+x\\s+)'),\n        }\n    \n    def preprocess_input_text(self, text: str) -> str:\n        if pd.isna(text):\n            return \"\"\n        text = str(text)\n        text = self.patterns['big_gap'].sub('<big_gap>', text)\n        text = self.patterns['small_gap'].sub('<gap>', text)\n        return text\n    \n    def preprocess_batch(self, texts: List[str]) -> List[str]:\n        s = pd.Series(texts).fillna('').astype(str)\n        s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n        s = s.str.replace(self.patterns['small_gap'], '<gap>', regex=True)\n        return s.tolist()\n\n\n# ============================================================\n# AKKADIAN CLAUSE-BOUNDARY CHUNKING (exact match: chunky_v1_5_0)\n# ============================================================\n\nCHUNK_MIN_WORDS = 15\nCHUNK_MAX_WORDS = 30\nCHUNK_THRESHOLD = 50\n\nCLAUSE_MARKERS = [\n    r'KIŠIB\\s+',\n    r'IGI\\s+',\n    r'um-ma\\s+',\n    r'a-na\\s+\\S+\\s+qí-bi',\n    r'šu-ma\\s+',\n    r'\\.\\s+',\n    r'\\[\\.\\.\\.\\]\\s*',\n]\nCLAUSE_PATTERN = re.compile('|'.join(CLAUSE_MARKERS), re.IGNORECASE)\n\n\ndef split_akkadian(text: str, max_words: int = CHUNK_MAX_WORDS, min_words: int = CHUNK_MIN_WORDS) -> List[str]:\n    words = text.split()\n    if len(words) <= CHUNK_THRESHOLD:\n        return [text]\n    \n    chunks, current_chunk = [], []\n    for word in words:\n        current_chunk.append(word)\n        chunk_text = ' '.join(current_chunk)\n        chunk_len = len(current_chunk)\n        is_break = bool(CLAUSE_PATTERN.search(chunk_text + ' '))\n        \n        if chunk_len >= min_words and is_break:\n            chunks.append(chunk_text.strip())\n            current_chunk = []\n        elif chunk_len >= max_words:\n            chunks.append(chunk_text.strip())\n            current_chunk = []\n    \n    if current_chunk:\n        last_chunk = ' '.join(current_chunk).strip()\n        if last_chunk:\n            chunks.append(last_chunk)\n    \n    return chunks if chunks else [text]\n\n\n# ============================================================\n# POSTPROCESSOR (exact match: chunky_v1_5_0)\n# ============================================================\n\ndef remove_phrase_repeats(text: str) -> str:\n    \"\"\"Remove repeated phrases of 3-8 words using sliding window.\"\"\"\n    if not text:\n        return text\n    words = text.split()\n    if len(words) < 6:\n        return text\n    for phrase_len in range(8, 2, -1):\n        i = 0\n        result_words = []\n        while i < len(words):\n            if i + phrase_len * 2 <= len(words):\n                phrase = words[i:i + phrase_len]\n                next_phrase = words[i + phrase_len:i + phrase_len * 2]\n                if phrase == next_phrase:\n                    result_words.extend(phrase)\n                    j = i + phrase_len\n                    while j + phrase_len <= len(words) and words[j:j + phrase_len] == phrase:\n                        j += phrase_len\n                    i = j\n                    continue\n            result_words.append(words[i])\n            i += 1\n        words = result_words\n    return ' '.join(words)\n\n\ndef trim_trailing_fragment(text: str) -> str:\n    \"\"\"Trim trailing incomplete word or sentence fragment.\"\"\"\n    if not text:\n        return text\n    text = text.rstrip()\n    if not text:\n        return text\n    if len(text) > 100 and text[-1].isalpha():\n        for i in range(len(text) - 1, -1, -1):\n            if text[i] in '.?!':\n                return text[:i + 1]\n            if text[i] in \"'\" and i > 0 and text[i - 1] in '.?!':\n                return text[:i + 1]\n    return text\n\n\nclass VectorizedPostprocessor:\n    def __init__(self, aggressive: bool = True):\n        self.aggressive = aggressive\n        self.patterns = {\n            'gap': re.compile(r'(\\[x\\]|\\(x\\)|\\bx\\b)', re.I),\n            'big_gap': re.compile(r'(\\.{3,}|…|\\[\\.+\\])'),\n            'annotations': re.compile(r'\\((fem|plur|pl|sing|singular|plural|\\?|!)\\..\\s*\\w*\\)', re.I),\n            'repeated_words': re.compile(r'\\b(\\w+)(?:\\s+\\1\\b)+'),\n            'whitespace': re.compile(r'\\s+'),\n            'punct_space': re.compile(r'\\s+([.,:])'),\n            'repeated_punct': re.compile(r'([.,])\\1+'),\n        }\n        self.subscript_trans = str.maketrans('₀₁₂₃₄₅₆₇₈₉', '0123456789')\n        self.special_chars_trans = str.maketrans('ḫḪ', 'hH')\n        self.forbidden_chars = '!?()\"——<>⌈⌋⌊[]+ʾ/;'\n        self.forbidden_trans = str.maketrans('', '', self.forbidden_chars)\n    \n    def postprocess_batch(self, translations: List[str]) -> List[str]:\n        s = pd.Series(translations)\n        valid_mask = s.apply(lambda x: isinstance(x, str) and x.strip())\n        if not valid_mask.all():\n            s[~valid_mask] = ''\n        \n        s = s.str.translate(self.special_chars_trans)\n        s = s.str.translate(self.subscript_trans)\n        s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n        s = s.str.strip()\n        \n        if self.aggressive:\n            s = s.str.replace(self.patterns['gap'], '<gap>', regex=True)\n            s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n            s = s.str.replace('<gap> <gap>', '<big_gap>', regex=False)\n            s = s.str.replace('<big_gap> <big_gap>', '<big_gap>', regex=False)\n            s = s.str.replace(self.patterns['annotations'], '', regex=True)\n            \n            s = s.str.replace('<gap>', '\\x00GAP\\x00', regex=False)\n            s = s.str.replace('<big_gap>', '\\x00BIG\\x00', regex=False)\n            s = s.str.translate(self.forbidden_trans)\n            s = s.str.replace('\\x00GAP\\x00', ' <gap> ', regex=False)\n            s = s.str.replace('\\x00BIG\\x00', ' <big_gap> ', regex=False)\n            \n            # Fractions (exact match: chunky_v1_5_0 — only ½, ¼, ¾)\n            s = s.str.replace(r'(\\d+)\\.5\\b', r'\\1½', regex=True)\n            s = s.str.replace(r'\\b0\\.5\\b', '½', regex=True)\n            s = s.str.replace(r'(\\d+)\\.25\\b', r'\\1¼', regex=True)\n            s = s.str.replace(r'\\b0\\.25\\b', '¼', regex=True)\n            s = s.str.replace(r'(\\d+)\\.75\\b', r'\\1¾', regex=True)\n            s = s.str.replace(r'\\b0\\.75\\b', '¾', regex=True)\n            \n            # Remove repeated words/n-grams\n            s = s.str.replace(self.patterns['repeated_words'], r'\\1', regex=True)\n            for n in range(4, 1, -1):\n                pattern = r'\\b((?:\\w+\\s+){' + str(n - 1) + r'}\\w+)(?:\\s+\\1\\b)+'\n                s = s.str.replace(pattern, r'\\1', regex=True)\n            \n            # Sliding-window phrase dedup\n            s = s.apply(remove_phrase_repeats)\n            \n            s = s.str.replace(self.patterns['punct_space'], r'\\1', regex=True)\n            s = s.str.replace(self.patterns['repeated_punct'], r'\\1', regex=True)\n            s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n            s = s.str.strip().str.strip('-').str.strip()\n            \n            # Trim trailing incomplete fragments\n            s = s.apply(trim_trailing_fragment)\n        \n        return s.tolist()\n\n\npreprocessor = OptimizedPreprocessor()\npostprocessor = VectorizedPostprocessor(aggressive=True)\nprint(\"Preprocessor and Postprocessor initialized (chunky_v1_5_0 exact match).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:47.883771Z","iopub.execute_input":"2026-02-11T16:24:47.884018Z","iopub.status.idle":"2026-02-11T16:24:47.911309Z","shell.execute_reply.started":"2026-02-11T16:24:47.883995Z","shell.execute_reply":"2026-02-11T16:24:47.910635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Loading\n","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/datasets/assiaben/final-byt5/byt5-akkadian-optimized-34x\"\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device).eval()\n\nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model loaded: {num_params:,} parameters on {device}\")\n\n# Apply BetterTransformer if available\ntry:\n    from optimum.bettertransformer import BetterTransformer\n    model = BetterTransformer.transform(model)\n    print(\"BetterTransformer applied\")\nexcept Exception as e:\n    print(f\"BetterTransformer skipped: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:24:47.912175Z","iopub.execute_input":"2026-02-11T16:24:47.912473Z","iopub.status.idle":"2026-02-11T16:25:42.616275Z","shell.execute_reply.started":"2026-02-11T16:24:47.912451Z","shell.execute_reply":"2026-02-11T16:25:42.615404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# DATASET & SAMPLER\n# ============================================================\n\nclass AkkadianDataset(Dataset):\n    def __init__(self, dataframe: pd.DataFrame, preprocessor):\n        if 'id' in dataframe.columns:\n            self.sample_ids = dataframe['id'].tolist()\n        else:\n            self.sample_ids = list(range(len(dataframe)))\n        raw_texts = dataframe['transliteration'].tolist()\n        preprocessed = preprocessor.preprocess_batch(raw_texts)\n        self.input_texts = ['translate Akkadian to English: ' + t for t in preprocessed]\n        print(f\"Dataset created: {len(self.sample_ids)} samples\")\n    \n    def __len__(self):\n        return len(self.sample_ids)\n    \n    def __getitem__(self, index):\n        return self.sample_ids[index], self.input_texts[index]\n\n\nclass BucketBatchSampler(Sampler):\n    def __init__(self, dataset, batch_size: int, num_buckets: int = 4):\n        lengths = [len(text.split()) for _, text in dataset]\n        sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n        bucket_size = max(1, len(sorted_indices) // num_buckets)\n        self.buckets = []\n        for i in range(num_buckets):\n            start = i * bucket_size\n            end = None if i == num_buckets - 1 else (i + 1) * bucket_size\n            self.buckets.append(sorted_indices[start:end])\n        self.batch_size = batch_size\n    \n    def __iter__(self):\n        for bucket in self.buckets:\n            for i in range(0, len(bucket), self.batch_size):\n                yield bucket[i:i + self.batch_size]\n    \n    def __len__(self):\n        return sum((len(b) + self.batch_size - 1) // self.batch_size for b in self.buckets)\n\nprint(\"Dataset and Sampler classes ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:25:42.617402Z","iopub.execute_input":"2026-02-11T16:25:42.618640Z","iopub.status.idle":"2026-02-11T16:25:42.627940Z","shell.execute_reply.started":"2026-02-11T16:25:42.618600Z","shell.execute_reply":"2026-02-11T16:25:42.627238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MOdel training","metadata":{}},{"cell_type":"code","source":"# Use a small sample for fast tuning\nVAL_SIZE = 100\nnp.random.seed(42)\nval_indices = np.random.choice(len(df_train), size=min(VAL_SIZE, len(df_train)), replace=False)\ndf_val = df_train.iloc[val_indices].reset_index(drop=True)\nprint(f\"Validation set: {len(df_val)} samples for Optuna tuning\")\n\n\ndef translate_batch_with_params(texts, length_penalty, num_beams, max_new_tokens=512):\n    \"\"\"Translate a list of texts with specific generation parameters.\"\"\"\n    preprocessed = preprocessor.preprocess_batch(texts)\n    prefixed = ['translate Akkadian to English: ' + t for t in preprocessed]\n    \n    translations = []\n    batch_size = 4\n    \n    with torch.inference_mode():\n        for i in range(0, len(prefixed), batch_size):\n            batch = prefixed[i:i + batch_size]\n            inputs = tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt')\n            input_ids = inputs.input_ids.to(device)\n            attention_mask = inputs.attention_mask.to(device)\n            \n            with autocast():\n                outputs = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    num_beams=num_beams,\n                    max_new_tokens=max_new_tokens,\n                    length_penalty=length_penalty,\n                    early_stopping=True,\n                    use_cache=True,\n                )\n            \n            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            translations.extend(decoded)\n    \n    cleaned = postprocessor.postprocess_batch(translations)\n    return cleaned\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:25:42.629700Z","iopub.execute_input":"2026-02-11T16:25:42.629934Z","iopub.status.idle":"2026-02-11T16:25:42.687778Z","shell.execute_reply.started":"2026-02-11T16:25:42.629911Z","shell.execute_reply":"2026-02-11T16:25:42.686866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef compute_bleu(predictions, references):\n    \"\"\"Corpus BLEU score.\"\"\"\n    #if USE_SACREBLEU:\n    return sacrebleu.corpus_bleu(predictions, [references]).score\n    #return _corpus_bleu_fallback(predictions, references)\n\n\ndef compute_chrf(predictions, references):\n    \"\"\"Corpus chrF++ score.\"\"\"\n    #if USE_SACREBLEU:\n    return sacrebleu.corpus_chrf(predictions, [references], word_order=2).score\n    #return _chrf_pp_fallback(predictions, references)\n\n\ndef compute_sentence_bleu(hypothesis, reference):\n    \"\"\"Sentence-level BLEU.\"\"\"\n    #if USE_SACREBLEU:\n    return sacrebleu.sentence_bleu(hypothesis, [reference]).score\n    #return _sentence_bleu_fallback(hypothesis, reference)\n\n\ndef compute_competition_score(predictions, references):\n    \"\"\"Compute geometric mean of BLEU and chrF++.\"\"\"\n    bleu_score = compute_bleu(predictions, references)\n    chrf_score = compute_chrf(predictions, references)\n    \n    if bleu_score <= 0 or chrf_score <= 0:\n        return 0.0\n    \n    return math.sqrt(bleu_score * chrf_score)\n\n\nprint(\"Translation and scoring functions ready.\")\nprint(f\"Metrics backend: {'sacrebleu'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:25:42.689063Z","iopub.execute_input":"2026-02-11T16:25:42.689566Z","iopub.status.idle":"2026-02-11T16:25:42.707011Z","shell.execute_reply.started":"2026-02-11T16:25:42.689539Z","shell.execute_reply":"2026-02-11T16:25:42.706297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna, sacrebleu, portalocker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:29:34.568305Z","iopub.execute_input":"2026-02-11T16:29:34.568907Z","iopub.status.idle":"2026-02-11T16:29:34.690100Z","shell.execute_reply.started":"2026-02-11T16:29:34.568875Z","shell.execute_reply":"2026-02-11T16:29:34.689101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Known good parameters from top public notebooks\nPROVEN_PARAMS = [\n    {'length_penalty': 1.5, 'num_beams': 8},   # chunky_v1_5_0 → 35.1\n    {'length_penalty': 1.3, 'num_beams': 8},   # adaptive-beams → 35.1\n]\n\ndef objective(trial):\n    length_penalty = trial.suggest_float('length_penalty', 0.8, 2.0)\n    num_beams = trial.suggest_int('num_beams', 4, 12)\n    \n    source_texts = df_val['transliteration'].tolist()\n    reference_texts = df_val['translation'].tolist()\n    \n    predictions = translate_batch_with_params(\n        source_texts,\n        length_penalty=length_penalty,\n        num_beams=num_beams,\n    )\n    \n    score = compute_competition_score(predictions, reference_texts)\n    return score\n\n\nstudy = optuna.create_study(direction='maximize')\n\n# Enqueue proven baselines so they are always evaluated first\nfor params in PROVEN_PARAMS:\n    study.enqueue_trial(params)\n\nstudy.optimize(objective, n_trials=50, timeout=3600 * 2)\n\n# Compare Optuna best vs proven baselines\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OPTUNA RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Best Score (geometric mean): {study.best_value:.2f}\")\nprint(f\"Best params: {study.best_params}\")\n\n# Show all trial results sorted by score\nprint(\"\\nAll trials (sorted by score):\")\ntrials_sorted = sorted(study.trials, key=lambda t: t.value if t.value is not None else 0, reverse=True)\nfor t in trials_sorted[:10]:\n    tag = \"\"\n    if t.params in PROVEN_PARAMS:\n        tag = \" [PROVEN BASELINE]\"\n    #print(f\"  Trial {t.number}: score={t.value:.2f}, lp={t.params['length_penalty']:.3f}, beams={t.params['num_beams']}{tag}\")\n#print(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:29:36.986434Z","iopub.execute_input":"2026-02-11T16:29:36.987441Z","iopub.status.idle":"2026-02-11T16:36:11.212733Z","shell.execute_reply.started":"2026-02-11T16:29:36.987407Z","shell.execute_reply":"2026-02-11T16:36:11.211931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Trial scores\ntrial_numbers = [t.number for t in study.trials]\ntrial_values = [t.value for t in study.trials]\naxes[0].plot(trial_numbers, trial_values, 'o-', color='#8b6914', markersize=8)\naxes[0].axhline(study.best_value, color='#d4a843', linestyle='--', label=f'Best: {study.best_value:.2f}')\naxes[0].set_xlabel('Trial Number')\naxes[0].set_ylabel('Score (Geometric Mean)')\naxes[0].set_title('Optuna Trial Scores', fontweight='bold')\naxes[0].legend()\n\n# Parameter importance - length_penalty vs score\nlp_values = [t.params['length_penalty'] for t in study.trials]\naxes[1].scatter(lp_values, trial_values, c='#8b6914', s=60, edgecolors='#2c1810')\naxes[1].set_xlabel('length_penalty')\naxes[1].set_ylabel('Score')\naxes[1].set_title('length_penalty vs Score', fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:36:46.601132Z","iopub.execute_input":"2026-02-11T16:36:46.601912Z","iopub.status.idle":"2026-02-11T16:36:46.913428Z","shell.execute_reply.started":"2026-02-11T16:36:46.601879Z","shell.execute_reply":"2026-02-11T16:36:46.912764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nFIXED_LENGTH_PENALTY = 1.5\nFIXED_NUM_BEAMS = 8\n\n# Show Optuna comparison\nprint(\"Optuna best vs proven baseline:\")\nprint(f\"  Optuna:  lp={study.best_params['length_penalty']:.3f}, beams={study.best_params['num_beams']}, score={study.best_value:.2f}\")\nprint(f\"  Proven:  lp={FIXED_LENGTH_PENALTY}, beams={FIXED_NUM_BEAMS}\")\n\n# Evaluate proven params on validation\nbest_length_penalty = FIXED_LENGTH_PENALTY\nbest_num_beams = FIXED_NUM_BEAMS\n\nval_predictions = translate_batch_with_params(\n    df_val['transliteration'].tolist(),\n    length_penalty=best_length_penalty,\n    num_beams=best_num_beams,\n)\nval_references = df_val['translation'].tolist()\n\nbleu_score = compute_bleu(val_predictions, val_references)\nchrf_score = compute_chrf(val_predictions, val_references)\ngeo_mean = math.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n\nprint(f\"\\nValidation Results (proven params: lp={best_length_penalty}, beams={best_num_beams}):\")\nprint(f\"  BLEU:  {bleu_score:.2f}\")\nprint(f\"  chrF++: {chrf_score:.2f}\")\nprint(f\"  Geometric Mean: {geo_mean:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:36:49.826718Z","iopub.execute_input":"2026-02-11T16:36:49.827308Z","iopub.status.idle":"2026-02-11T16:40:07.631516Z","shell.execute_reply.started":"2026-02-11T16:36:49.827277Z","shell.execute_reply":"2026-02-11T16:40:07.630772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart of metrics\nmetrics = ['BLEU', 'chrF++', 'Geometric Mean']\nvalues = [bleu_score, chrf_score, geo_mean]\ncolors = ['#2c1810', '#8b6914', '#d4a843']\nbars = axes[0].bar(metrics, values, color=colors, edgecolor='#2c1810', linewidth=1.5)\nfor bar, val in zip(bars, values):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                f'{val:.2f}', ha='center', fontweight='bold', fontsize=12)\naxes[0].set_ylabel('Score')\naxes[0].set_title('Validation Metrics', fontweight='bold')\naxes[0].set_ylim(0, max(values) * 1.2)\n\n# Per-sample BLEU distribution\nper_sample_bleu = []\nfor pred, ref in zip(val_predictions, val_references):\n    s = compute_sentence_bleu(pred, ref)\n    per_sample_bleu.append(s)\n\naxes[1].hist(per_sample_bleu, bins=30, color='#8b6914', alpha=0.7, edgecolor='#2c1810')\naxes[1].axvline(np.mean(per_sample_bleu), color='#d4a843', linestyle='--', linewidth=2,\n                label=f'Mean: {np.mean(per_sample_bleu):.1f}')\naxes[1].set_xlabel('Sentence BLEU')\naxes[1].set_ylabel('Count')\naxes[1].set_title('Per-Sample BLEU Distribution', fontweight='bold')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:40:07.633008Z","iopub.execute_input":"2026-02-11T16:40:07.633344Z","iopub.status.idle":"2026-02-11T16:40:08.021045Z","shell.execute_reply.started":"2026-02-11T16:40:07.633305Z","shell.execute_reply":"2026-02-11T16:40:08.020352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Sample Predictions vs References:\")\nfor i in range(min(5, len(val_predictions))):\n    s_bleu = compute_sentence_bleu(val_predictions[i], val_references[i])\n    print(\"=\" * 70)\n    print(f\"[{i}] BLEU: {s_bleu:.1f}\")\n    print(f\"SRC:  {df_val.iloc[i]['transliteration'][:150]}\")\n    print(f\"REF:  {val_references[i][:150]}\")\n    print(f\"PRED: {val_predictions[i][:150]}\")\nprint(\"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:40:08.022155Z","iopub.execute_input":"2026-02-11T16:40:08.022512Z","iopub.status.idle":"2026-02-11T16:40:08.033975Z","shell.execute_reply.started":"2026-02-11T16:40:08.022486Z","shell.execute_reply":"2026-02-11T16:40:08.033160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nBATCH_SIZE = 8\nMAX_LENGTH = 512\nNUM_WORKERS = 4\nNUM_BUCKETS = 4\n\nprint(f\"Inference config: length_penalty={best_length_penalty}, num_beams={best_num_beams}\")\nprint(f\"Test samples: {len(df_test)}\")\n\ntest_dataset = AkkadianDataset(df_test, preprocessor)\n\ndef collate_fn(batch):\n    ids = [s[0] for s in batch]\n    texts = [s[1] for s in batch]\n    tokenized = tokenizer(texts, max_length=MAX_LENGTH, padding=True, truncation=True, return_tensors='pt')\n    return ids, tokenized\n\nprint(f\"Dataset ready: {len(test_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:40:08.035448Z","iopub.execute_input":"2026-02-11T16:40:08.035685Z","iopub.status.idle":"2026-02-11T16:40:08.054776Z","shell.execute_reply.started":"2026-02-11T16:40:08.035661Z","shell.execute_reply":"2026-02-11T16:40:08.054046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nresults = []\nchunked_ids = set()\n\ngen_config_chunk = {\n    'num_beams': best_num_beams,\n    'max_new_tokens': 512,\n    'length_penalty': best_length_penalty,\n    'early_stopping': True,\n    'use_cache': True,\n}\n\nprint(\"Phase 1: Translating long texts with clause-boundary chunking...\")\n\nwith torch.inference_mode():\n    for idx in range(len(test_dataset)):\n        sample_id, input_text = test_dataset[idx]\n        raw_text = input_text.replace('translate Akkadian to English: ', '')\n        \n        if len(raw_text.split()) > CHUNK_THRESHOLD:\n            chunks = split_akkadian(raw_text)\n            prefix = 'translate Akkadian to English: '\n            chunk_translations = []\n            \n            for chunk in chunks:\n                inputs = tokenizer(prefix + chunk, return_tensors='pt',\n                                  max_length=MAX_LENGTH, truncation=True).to(device)\n                if torch.cuda.is_available():\n                    with autocast():\n                        outputs = model.generate(\n                            input_ids=inputs.input_ids,\n                            attention_mask=inputs.attention_mask,\n                            **gen_config_chunk\n                        )\n                else:\n                    outputs = model.generate(\n                        input_ids=inputs.input_ids,\n                        attention_mask=inputs.attention_mask,\n                        **gen_config_chunk\n                    )\n                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                chunk_translations.append(translation.strip())\n            \n            full_translation = ' '.join(chunk_translations)\n            cleaned = postprocessor.postprocess_batch([full_translation])[0]\n            results.append((sample_id, cleaned))\n            chunked_ids.add(idx)\n\nprint(f\"Chunked {len(chunked_ids)} long texts\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:40:08.055588Z","iopub.execute_input":"2026-02-11T16:40:08.055810Z","iopub.status.idle":"2026-02-11T16:40:08.071851Z","shell.execute_reply.started":"2026-02-11T16:40:08.055789Z","shell.execute_reply":"2026-02-11T16:40:08.071293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"Phase 2: Batch translating remaining texts...\")\n\nif chunked_ids:\n    short_indices = [i for i in range(len(test_dataset)) if i not in chunked_ids]\n    short_dataset = torch.utils.data.Subset(test_dataset, short_indices)\nelse:\n    short_dataset = test_dataset\n\nif len(short_dataset) > 0:\n    if len(short_dataset) >= NUM_BUCKETS:\n        batch_sampler_short = BucketBatchSampler(short_dataset, BATCH_SIZE, NUM_BUCKETS)\n        dataloader_short = DataLoader(\n            short_dataset, batch_sampler=batch_sampler_short,\n            collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True,\n            prefetch_factor=2, persistent_workers=True if NUM_WORKERS > 0 else False\n        )\n    else:\n        dataloader_short = DataLoader(\n            short_dataset, batch_size=BATCH_SIZE, shuffle=False,\n            collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True,\n            prefetch_factor=2, persistent_workers=True if NUM_WORKERS > 0 else False\n        )\n    \n    base_gen_config = {\n        'max_new_tokens': 512,\n        'length_penalty': best_length_penalty,\n        'early_stopping': True,\n        'use_cache': True,\n    }\n    \n    with torch.inference_mode():\n        for batch_idx, (batch_ids, tokenized) in enumerate(tqdm(dataloader_short, desc=\"Translating\")):\n            input_ids = tokenized.input_ids.to(device)\n            attention_mask = tokenized.attention_mask.to(device)\n            \n            # Adaptive beams (exact match: chunky_v1_5_0)\n            lengths = attention_mask.sum(dim=1)\n            beam_sizes = torch.where(\n                lengths < 100,\n                torch.tensor(max(4, best_num_beams // 2)),\n                torch.tensor(best_num_beams),\n            )\n            adaptive_beams = int(beam_sizes[0].item())\n            \n            gen_config = {**base_gen_config, 'num_beams': adaptive_beams}\n            \n            if torch.cuda.is_available():\n                with autocast():\n                    outputs = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        **gen_config\n                    )\n            else:\n                outputs = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    **gen_config\n                )\n            \n            translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            cleaned = postprocessor.postprocess_batch(translations)\n            results.extend(zip(batch_ids, cleaned))\n            \n            if torch.cuda.is_available() and batch_idx % 10 == 0:\n                torch.cuda.empty_cache()\n\nprint(f\"\\nTotal translations: {len(results)}\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:40:33.492590Z","iopub.execute_input":"2026-02-11T16:40:33.492923Z","iopub.status.idle":"2026-02-11T16:40:44.119321Z","shell.execute_reply.started":"2026-02-11T16:40:33.492889Z","shell.execute_reply":"2026-02-11T16:40:44.118025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build submission\nsubmission = pd.DataFrame(results, columns=['id', 'translation'])\nsubmission = submission.sort_values('id').reset_index(drop=True)\nsubmission = submission.drop_duplicates()\n\nprint(submission)\n\n# Validation checks\nassert len(submission) == len(df_test), f\"Expected {len(df_test)} rows, got {len(submission)}\"\nempty_count = submission['translation'].str.strip().eq('').sum()\nprint(f\"Submission shape: {submission.shape}\")\nprint(f\"Empty translations: {empty_count}\")\nprint(f\"Translation length range: [{submission['translation'].str.len().min()}, {submission['translation'].str.len().max()}]\")\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"\\nSaved submission.csv\")\nsubmission.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:41:19.380889Z","iopub.execute_input":"2026-02-11T16:41:19.381284Z","iopub.status.idle":"2026-02-11T16:41:19.414050Z","shell.execute_reply.started":"2026-02-11T16:41:19.381205Z","shell.execute_reply":"2026-02-11T16:41:19.413186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}